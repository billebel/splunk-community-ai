# Multi-LLM Context Window Configuration
# Adaptive pagination and summarization based on LLM capabilities

llm_profiles:
  # Claude Models
  claude-3-haiku:
    context_window: 200000
    chars_per_token: 4
    reserved_tokens: 3000
    optimal_usage: 0.7  # Use 70% of context for data
    token_efficiency: "high"
    best_for: ["fast_queries", "simple_analysis"]
    
  claude-3-sonnet:
    context_window: 200000
    chars_per_token: 4
    reserved_tokens: 4000
    optimal_usage: 0.75
    token_efficiency: "high"
    best_for: ["complex_analysis", "investigation", "discovery"]
    
  claude-3-opus:
    context_window: 200000
    chars_per_token: 4
    reserved_tokens: 5000
    optimal_usage: 0.8
    token_efficiency: "high"
    best_for: ["deep_analysis", "pattern_detection", "complex_investigation"]

  # GPT Models
  gpt-4-turbo:
    context_window: 128000
    chars_per_token: 4
    reserved_tokens: 4000
    optimal_usage: 0.7
    token_efficiency: "medium"
    best_for: ["general_analysis", "code_generation"]
    
  gpt-4:
    context_window: 8192
    chars_per_token: 4
    reserved_tokens: 2000
    optimal_usage: 0.6
    token_efficiency: "medium"
    best_for: ["focused_analysis", "small_datasets"]
    
  gpt-3.5-turbo:
    context_window: 16384
    chars_per_token: 4
    reserved_tokens: 2000
    optimal_usage: 0.6
    token_efficiency: "medium"
    best_for: ["quick_queries", "simple_stats"]

  # Gemini Models
  gemini-1.5-pro:
    context_window: 2000000  # 2M tokens!
    chars_per_token: 4
    reserved_tokens: 10000
    optimal_usage: 0.85
    token_efficiency: "very_high"
    best_for: ["massive_datasets", "comprehensive_analysis", "full_context"]
    
  gemini-1.5-flash:
    context_window: 1000000  # 1M tokens
    chars_per_token: 4
    reserved_tokens: 5000
    optimal_usage: 0.8
    token_efficiency: "very_high"
    best_for: ["large_datasets", "fast_processing"]
    
  gemini-pro:
    context_window: 32768
    chars_per_token: 4
    reserved_tokens: 3000
    optimal_usage: 0.7
    token_efficiency: "high"
    best_for: ["standard_analysis", "balanced_processing"]

  # Ollama/Local Models (variable based on hardware)
  ollama-llama3-70b:
    context_window: 8192
    chars_per_token: 4
    reserved_tokens: 1500
    optimal_usage: 0.6
    token_efficiency: "low"
    best_for: ["privacy_focused", "local_processing"]
    
  ollama-llama3-8b:
    context_window: 8192
    chars_per_token: 4
    reserved_tokens: 1000
    optimal_usage: 0.5
    token_efficiency: "low"
    best_for: ["quick_local_analysis", "resource_constrained"]
    
  ollama-mistral-7b:
    context_window: 8192
    chars_per_token: 4
    reserved_tokens: 1000
    optimal_usage: 0.5
    token_efficiency: "low"
    best_for: ["local_development", "testing"]

# Query Type Strategies per LLM Class
strategy_matrix:
  large_context:  # Gemini 1.5 Pro/Flash
    statistical:
      max_results: 1000
      include_raw_events: true
      summarization_level: "minimal"
      
    investigation:
      max_samples: 50
      pattern_detection: "comprehensive"
      include_full_events: true
      
    discovery:
      field_analysis_depth: "complete"
      sample_events: 20
      value_analysis: "comprehensive"
      
    general:
      smart_sampling: 30
      diversity_analysis: true
      full_field_context: true

  medium_context:  # Claude, GPT-4 Turbo
    statistical:
      max_results: 200
      include_raw_events: false
      summarization_level: "moderate"
      
    investigation:
      max_samples: 15
      pattern_detection: "focused"
      include_key_fields: true
      
    discovery:
      field_analysis_depth: "important_fields"
      sample_events: 8
      value_analysis: "top_values"
      
    general:
      smart_sampling: 12
      diversity_analysis: true
      key_fields_only: true

  small_context:  # GPT-4, Ollama models
    statistical:
      max_results: 50
      include_raw_events: false
      summarization_level: "aggressive"
      
    investigation:
      max_samples: 5
      pattern_detection: "basic"
      essential_fields_only: true
      
    discovery:
      field_analysis_depth: "top_fields"
      sample_events: 3
      value_analysis: "summary_only"
      
    general:
      smart_sampling: 5
      diversity_analysis: false
      minimal_fields: true

# Auto-detection patterns
llm_detection:
  user_agent_patterns:
    - pattern: "claude"
      default_profile: "claude-3-sonnet"
    - pattern: "gpt-4-turbo"
      default_profile: "gpt-4-turbo"
    - pattern: "gpt-4"
      default_profile: "gpt-4"
    - pattern: "gpt-3.5"
      default_profile: "gpt-3.5-turbo"
    - pattern: "gemini.*1.5.*pro"
      default_profile: "gemini-1.5-pro"
    - pattern: "gemini.*1.5.*flash"
      default_profile: "gemini-1.5-flash"
    - pattern: "gemini"
      default_profile: "gemini-pro"
    - pattern: "ollama"
      default_profile: "ollama-llama3-8b"
      
  api_endpoint_patterns:
    - pattern: "anthropic"
      default_profile: "claude-3-sonnet"
    - pattern: "openai"
      default_profile: "gpt-4-turbo" 
    - pattern: "google.*ai"
      default_profile: "gemini-1.5-pro"
    - pattern: "localhost.*11434"
      default_profile: "ollama-llama3-8b"

# Fallback configuration
fallback:
  profile: "gpt-4"
  reason: "Conservative default for unknown LLM"
  
# Performance tuning
optimization:
  token_estimation_buffer: 0.1  # 10% buffer for estimation errors
  compression_threshold: 0.9    # Compress if using >90% of available tokens  
  progressive_reduction: true   # Reduce data quality before failing
  quality_levels: ["full", "reduced", "minimal", "summary_only"]