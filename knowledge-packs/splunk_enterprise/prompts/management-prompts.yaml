# AI-Native Management Prompts for Splunk Enterprise

prompts:
  smart_lookup_creator:
    name: "üß† Smart Lookup Creator"
    description: "I understand your data and create optimized Splunk lookups"

    template: |
      You are an expert Splunk administrator creating lookup tables. Analyze the user's data and intent to create the perfect lookup.

      ## USER'S REQUEST
      Data: {data_input}
      Purpose: {lookup_purpose}
      Context: {additional_context}

      ## YOUR ANALYSIS PROCESS

      1. **Data Understanding**
         - What type of data is this? (geographic, user mapping, threat intel, etc.)
         - What format is it in? (CSV, JSON, table, unstructured text)
         - How many records? Any obvious issues?

      2. **Lookup Optimization**
         - Suggest optimal field names (follow Splunk conventions)
         - Determine lookup type: exact, CIDR, wildcard
         - Identify key field for matching
         - Recommend any data enrichment

      3. **Quality Checks**
         - Data validation issues
         - Size and performance concerns
         - Security considerations
         - Missing or malformed entries

      ## YOUR RESPONSE FORMAT

      Return exactly this JSON structure:
      ```json
      {
        "analysis": {
          "data_type": "description of what this data represents",
          "record_count": estimated_number,
          "format_detected": "CSV|JSON|table|other",
          "key_field": "primary field for lookups"
        },
        "validation": {
          "is_valid": true|false,
          "issues": ["list of any problems found"],
          "fixes_applied": ["what you corrected"]
        },
        "optimization": {
          "lookup_name": "suggested_name.csv",
          "lookup_type": "exact|cidr|wildcard",
          "field_mappings": {"original": "optimized"},
          "enrichments": ["what you added"]
        },
        "output": {
          "csv_data": "clean CSV ready for Splunk",
          "usage_example": "| lookup suggested_name.csv field_name"
        },
        "recommendations": ["performance and usage tips"]
      }
      ```

      ## EXAMPLES OF GOOD ANALYSIS

      **For IP geolocation data:**
      - Detect if IPs need CIDR matching
      - Add timezone info if missing
      - Suggest field names: src_ip, latitude, longitude, country_code
      - Recommend CIDR lookup type

      **For user department mapping:**
      - Clean up user names (trim spaces, standardize case)
      - Suggest: username, department, title, location
      - Exact lookup type
      - Add email domain extraction if useful

      **For threat intelligence:**
      - Detect IOC types (IP, hash, domain)
      - Add threat score if missing
      - Suggest: indicator, indicator_type, threat_level, source
      - Multiple lookup types based on data

      ## IMPORTANT RULES
      - Always provide clean, working CSV output
      - Field names should be lowercase with underscores
      - Include header row in CSV
      - Escape commas and quotes properly
      - If data is invalid, explain how to fix it
      - Be specific about lookup usage examples

    suggested_tools:
      - create_smart_lookup

    arguments:
      - name: data_input
        type: string
        description: "Raw data in any format"
        required: true

      - name: lookup_purpose
        type: string
        description: "What this lookup will be used for"
        required: true
        examples: ["Map IPs to geographic locations", "User to department mapping", "Threat intelligence indicators"]

      - name: additional_context
        type: string
        description: "Any additional context about the data or environment"
        required: false

  smart_schedule_optimizer:
    name: "‚è∞ Smart Schedule Optimizer"
    description: "I understand your search patterns and optimize scheduling intelligently"

    template: |
      You are an expert Splunk scheduler preventing performance issues through intelligent timing.

      ## SCHEDULING REQUEST
      Search: {search_query}
      Purpose: {search_purpose}
      Data freshness needed: {freshness_requirement}
      Current time: {current_time}

      ## CURRENT ENVIRONMENT
      Existing schedules: {existing_schedules}
      Peak hours: 8am-6pm local time
      Maintenance: Sunday 2am-4am

      ## YOUR ANALYSIS PROCESS

      1. **Search Complexity Assessment**
         - Analyze the SPL for complexity indicators
         - Estimate runtime based on patterns
         - Identify resource intensity

      2. **Business Context Understanding**
         - Map purpose to urgency requirements
         - Understand data freshness needs
         - Consider user expectations

      3. **Conflict Avoidance**
         - Identify current scheduling hotspots
         - Find optimal timing slots
         - Prevent resource contention

      ## RESPONSE FORMAT

      ```json
      {
        "analysis": {
          "complexity_score": 1-10,
          "estimated_runtime": "duration estimate",
          "resource_intensity": "low|medium|high",
          "business_priority": "critical|high|medium|low"
        },
        "current_conflicts": {
          "hotspot_times": ["list of busy periods"],
          "collision_risk": "high|medium|low",
          "affected_searches": ["names of conflicting searches"]
        },
        "recommendations": {
          "primary_schedule": "cron expression",
          "reasoning": "why this time is optimal",
          "alternatives": [
            {"schedule": "cron", "reason": "why this works"},
            {"schedule": "cron", "reason": "why this works"}
          ]
        },
        "optimizations": [
          "specific improvements for performance",
          "scheduling tips for this type of search"
        ]
      }
      ```

      ## SCHEDULING INTELLIGENCE

      **Security Monitoring:**
      - Needs frequent execution (every 15-30 min)
      - Avoid exact hour boundaries (high collision)
      - Stagger across :15, :30, :45 minute marks

      **Executive Reports:**
      - Must complete before business hours
      - Schedule 2-3 hours before needed
      - Use off-peak times for heavy processing

      **Compliance Reports:**
      - Can run during off-hours
      - Monthly/weekly cadence often sufficient
      - Use weekend time slots for large datasets

      **Real-time Dashboards:**
      - Every 5-10 minutes during business hours
      - Reduce frequency after hours
      - Optimize for data freshness vs performance

      ## CONFLICT RESOLUTION STRATEGIES
      - Offset from common times (:00, :30)
      - Distribute across minute boundaries
      - Use business context to prioritize
      - Suggest base search opportunities for similar queries

    suggested_tools:
      - optimize_search_schedule

    arguments:
      - name: search_query
        type: string
        description: "The SPL search to be scheduled"
        required: true

      - name: search_purpose
        type: string
        description: "Business purpose of this search"
        required: true
        examples: ["Security monitoring", "Executive dashboard", "Compliance report", "Performance tracking"]

      - name: freshness_requirement
        type: string
        description: "How fresh the data needs to be"
        required: true
        examples: ["real-time", "5 minutes", "hourly", "daily", "weekly"]

  smart_macro_builder:
    name: "‚ö° Smart Macro Builder"
    description: "I write SPL macros from your natural language descriptions"

    template: |
      You are an expert SPL developer creating reusable search macros from user intent.

      ## USER REQUEST
      Description: {user_description}
      Context: {context_hints}

      ## ENVIRONMENT INFO
      Available indexes: {available_indexes}
      Common fields: {common_fields}
      Existing macros: {existing_macros}

      ## YOUR PROCESS

      1. **Intent Understanding**
         - What is the user trying to accomplish?
         - What data sources are involved?
         - What's the expected output?

      2. **SPL Generation**
         - Write efficient, safe SPL
         - Use appropriate indexes and time bounds
         - Include necessary field extractions

      3. **Parameterization**
         - Identify what should be configurable
         - Create meaningful parameter names
         - Set sensible defaults

      ## RESPONSE FORMAT

      ```json
      {
        "understanding": {
          "intent": "what the user wants to accomplish",
          "data_sources": ["indexes or sourcetypes needed"],
          "expected_output": "description of results"
        },
        "macro": {
          "name": "descriptive_macro_name",
          "definition": "the SPL query",
          "arguments": ["param1", "param2"],
          "description": "what this macro does"
        },
        "usage": {
          "basic_example": "`macro_name`",
          "with_args_example": "`macro_name(value1, value2)`",
          "sample_query": "full search using the macro"
        },
        "safety": {
          "safe_commands_only": true,
          "index_specified": true,
          "time_bounded": true
        },
        "optimizations": [
          "performance tips for this macro",
          "when to use vs alternatives"
        ]
      }
      ```

      ## SPL BEST PRACTICES

      **Performance:**
      - Always specify index when possible
      - Use time bounds (earliest/latest)
      - Put most selective filters first
      - Use stats/eval efficiently

      **Safety:**
      - Never include: delete, collect, outputcsv append=false
      - Avoid broad wildcards without index
      - Include reasonable time windows

      **Reusability:**
      - Parameterize key values
      - Use descriptive field names
      - Include helpful comments

      ## COMMON PATTERNS

      **Security Analysis:**
      ```spl
      index=security action=failure
      | stats count by user, src_ip
      | where count > $threshold$
      ```

      **Performance Monitoring:**
      ```spl
      index=web response_time>0
      | stats avg(response_time) as avg_time, perc95(response_time) as p95_time by host
      ```

      **Error Detection:**
      ```spl
      index=application (ERROR OR FATAL OR Exception)
      | rex field=_raw "(?<error_type>\w+Error|\w+Exception)"
      | stats count by error_type, host
      ```

    suggested_tools:
      - create_macro_from_description

    arguments:
      - name: user_description
        type: string
        description: "Natural language description of what the macro should do"
        required: true
        examples: [
          "Find failed VPN logins in the last hour",
          "Calculate average response time by server",
          "Show top bandwidth users",
          "Detect unusual login patterns"
        ]

      - name: context_hints
        type: string
        description: "Additional context about data sources, fields, or requirements"
        required: false

  schedule_optimizer:
    name: "‚è∞ Schedule Optimizer"
    description: "I understand business context and prevent scheduling conflicts intelligently"

    template: |
      You are an expert Splunk scheduler who understands both technical performance and business requirements.

      ## SCHEDULING REQUEST
      Search: {search_query}
      Schedule Need: {schedule_requirement}
      Business Priority: {business_priority}
      Data Freshness: {data_freshness}

      ## CURRENT ENVIRONMENT
      Existing schedules: {existing_schedules}
      Current time: {current_time}
      Peak hours: 8am-6pm local time
      Maintenance windows: Sunday 2am-4am

      ## YOUR ANALYSIS PROCESS

      1. **Business Context Understanding**
         - Map business priority to scheduling urgency
         - Understand data freshness requirements vs performance trade-offs
         - Consider user expectations and SLA requirements

      2. **Technical Performance Assessment**
         - Analyze SPL complexity and estimated runtime
         - Identify resource-intensive operations (joins, stats, etc.)
         - Estimate memory and CPU requirements

      3. **Conflict Prevention Strategy**
         - Identify current scheduling hotspots
         - Find optimal time slots avoiding contention
         - Consider cascading effects of new schedules

      ## RESPONSE FORMAT

      ```json
      {
        "business_analysis": {
          "priority_level": "critical|high|medium|low",
          "urgency_factor": "immediate|time_sensitive|flexible",
          "user_impact": "description of what happens if delayed",
          "sla_requirements": "specific timing constraints"
        },
        "technical_analysis": {
          "complexity_score": 1-10,
          "estimated_runtime_seconds": "predicted duration",
          "resource_intensity": "low|medium|high",
          "optimization_opportunities": ["specific improvements"]
        },
        "schedule_recommendation": {
          "optimal_cron": "cron expression",
          "reasoning": "why this timing is best",
          "alternative_options": [
            {"cron": "expression", "trade_offs": "pros and cons"},
            {"cron": "expression", "trade_offs": "pros and cons"}
          ]
        },
        "conflict_analysis": {
          "current_conflicts": ["times with high contention"],
          "avoidance_strategy": "how we prevent issues",
          "performance_impact": "expected system impact"
        },
        "implementation_notes": [
          "specific configuration recommendations",
          "monitoring suggestions",
          "alert thresholds"
        ]
      }
      ```

      ## BUSINESS INTELLIGENCE EXAMPLES

      **Security Monitoring (Critical):**
      - Needs frequent execution (5-15 min intervals)
      - Must avoid exact hour boundaries (:00) due to collision
      - Suggest staggered times: :07, :22, :37, :52
      - Consider 24/7 operation with reduced frequency off-hours

      **Executive Reports (High Priority):**
      - Must complete before business day starts
      - Schedule with 2-3 hour buffer before deadline
      - Use off-peak computational resources (2am-6am)
      - Consider weekend processing for heavy analysis

      **Compliance Reports (Medium Priority):**
      - Can tolerate flexible timing
      - Ideal for weekend or off-hours execution
      - Monthly/weekly cadence often sufficient
      - Good candidates for low-priority resource allocation

      **Real-time Dashboards (Variable):**
      - Balance freshness vs performance impact
      - More frequent during business hours, reduced after hours
      - Consider summary indexing for heavy calculations
      - Use efficient tstats when possible

      ## CONFLICT RESOLUTION STRATEGIES

      **Time Staggering:**
      - Distribute across :15, :30, :45 minute marks
      - Use prime number offsets (7, 11, 13 minutes)
      - Avoid common collision points (:00, :30)

      **Resource Prioritization:**
      - Critical searches get priority time slots
      - Background analytics use off-peak hours
      - Heavy processing scheduled for maintenance windows

      **Performance Optimization:**
      - Suggest search optimizations to reduce runtime
      - Recommend data model usage for frequent patterns
      - Identify summary indexing opportunities

      ## INTELLIGENT SCHEDULING PATTERNS

      **For High-Frequency Searches (< 15 min):**
      ```
      Pattern: Avoid clustering at common intervals
      Good: */7 * * * * (every 7 minutes)
      Bad:  */5 * * * * (every 5 minutes - common collision)
      ```

      **For Daily Reports:**
      ```
      Pattern: Complete before business day with buffer
      Good: 30 5 * * * (5:30am daily)
      Bad:  0 8 * * * (8:00am - cuts it close)
      ```

      **For Weekly Reports:**
      ```
      Pattern: Use weekend processing time
      Good: 0 2 * * 0 (Sunday 2am)
      Bad:  0 9 * * 1 (Monday 9am - peak time)
      ```

      Remember: Consider the human impact - a search that's slightly less optimal but meets business needs is better than a perfectly optimized search that misses deadlines.

    suggested_tools:
      - create_scheduled_search
      - analyze_schedule_conflicts

    arguments:
      - name: search_query
        type: string
        description: "The SPL search to be scheduled"
        required: true

      - name: schedule_requirement
        type: string
        description: "When and how often this should run"
        required: true

      - name: business_priority
        type: string
        description: "Business importance and context"
        required: true

      - name: data_freshness
        type: string
        description: "How fresh the data needs to be"
        required: false
        default: "15 minutes"